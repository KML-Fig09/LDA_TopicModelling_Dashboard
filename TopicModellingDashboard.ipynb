{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **üöÄ Notebook to extract text from pdf (if needed), build an LDA model, save it, set up a pyLDAvis visual, and then write a Streamlit file.**\n"
      ],
      "metadata": {
        "id": "DwSOk2iE_0tv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üõ† Install and Configure Environment\n",
        "\n",
        "This section sets up the Python environment by installing specific package versions needed for topic modelling.\n",
        "\n",
        "**Important:**  \n",
        "- Some Colab default packages conflict with `pyLDAvis` and `gensim`.\n",
        "- We first uninstall any conflicting packages, then install the correct versions.\n",
        "\n",
        "‚úÖ **The runtime will say it has crashed after this step. This is normal and part of the code - just continue to the next block and run it.**\n"
      ],
      "metadata": {
        "id": "7GYccF5G_chL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o5JPyUcy3Fq"
      },
      "outputs": [],
      "source": [
        "# Install required libraries for topic modeling and visualisation:\n",
        "# - gensim: for topic modelling using LDA (Latent Dirichlet Allocation) to discover hidden topics\n",
        "# - nltk: for text cleaning and tokenization\n",
        "# - pyLDAvis: for interactive topic visualisation\n",
        "# - tqdm: for progress bars during data loading\n",
        "\n",
        "# Setup environment for Gensim + pyLDAvis compatibility in Colab (encountered issues with this previously)\n",
        "# This ensures NumPy is downgraded before Gensim is installed\n",
        "\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "# ‚úÖ Reset environment, uninstall problematic packages first\n",
        "!pip uninstall -y numpy scipy scikit-learn gensim pyldavis tensorflow thinc blosc2 imbalanced-learn\n",
        "\n",
        "# üßπ Reinstall compatible versions (lock to known working versions)\n",
        "!pip install numpy==1.25.2 scipy==1.10.1 scikit-learn==1.3.1 --quiet\n",
        "!pip install gensim==4.3.1 pyLDAvis==3.4.1 tqdm nltk pandas pymupdf --quiet\n",
        "\n",
        "# üîÅ Restart runtime AFTER THIS CELL RUNS\n",
        "import os\n",
        "print(\"‚úÖ All packages reinstalled successfully. Now restarting the runtime...\")\n",
        "os.kill(os.getpid(), 9)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìö Import Libraries and Configure Logging\n",
        "\n",
        "We now import all the essential libraries needed for:\n",
        "- Text preprocessing\n",
        "- Topic modelling\n",
        "- Visualisation\n",
        "- File handling\n",
        "\n",
        "Logging is also configured to help monitor progress and spot errors.\n"
      ],
      "metadata": {
        "id": "LxnL81Z_AfnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries and set up logging for review\n",
        "\n",
        "# Libraries:\n",
        "import os\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from gensim import corpora, models\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "# Logging (time that the log was made, log type, and the log message):\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')"
      ],
      "metadata": {
        "id": "Nmv35enu0GlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîó Mount Google Drive\n",
        "\n",
        "Mount your Google Drive to Colab to access:\n",
        "- Your input documents (PDFs or extracted `.txt` files)\n",
        "- Folders where outputs (models, CSVs, visualisations) will be saved, etc.\n",
        "\n",
        "**Action:**  \n",
        "- Click \"Connect to Google Drive\" when prompted.\n",
        "- Replace \"/YOUR/GDRIVE/PATH/HERE\" with the GDrive path you want your outputs to be saved at."
      ],
      "metadata": {
        "id": "BhT4PU7nAlqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount GDrive to access both the input text file database and to save the output files\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the folder where .txt files are located\n",
        "output_folder = Path(\"/YOUR/GDRIVE/PATH/HERE\")\n",
        "output_folder.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "lIrYkfoc1_1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÑ (Optional) Extract Text from PDF Files\n",
        "\n",
        "If you start with PDFs (not `.txt` files), the following modules in this section will:\n",
        "- Define file paths.\n",
        "- Extract text from each PDF file.\n",
        "- Save them as `.txt` files into your output folder.\n",
        "\n",
        "**Note:**  \n",
        "- Only run this if you have raw PDF files.\n",
        "- Otherwise, skip ahead if you already have `.txt` documents.\n",
        "- Replace \"/YOUR/GDRIVE/PATH/HERE\" with the GDrive paths leading to where you have your pdf dataset saved, as well as where you want the extracted files to be saved."
      ],
      "metadata": {
        "id": "K5q_jRkjA1k2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define file paths: Unneeded if you've already run the prior step AND you're NOT extracting new text.\n",
        "# If you are extracting new text, make sure to run this path & replace the input folder with the correct path to your DS.\n",
        "\n",
        "#Define data input & output paths to destination directories using pathlib\n",
        "\n",
        "#Input: Folder containing source PDFs (test first with a small batch <100, larger batches will take a longer time)\n",
        "\n",
        "pdf_input_folder = Path(\"/YOUR/GDRIVE/PATH/HERE\")\n",
        "\n",
        "#Output: Folder to store the corresponding extracted .txt files for each PDF.\n",
        "\n",
        "output_folder = Path(\"/YOUR/GDRIVE/PATH/HERE\")\n",
        "output_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#logging\n",
        "\n",
        "logging.info(f\"Saving extracted texts to {output_folder}\")"
      ],
      "metadata": {
        "id": "PUmVGCcgCgL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Extract .txt files from PDF files. I'm doing this step again because I want a larger dataset\n",
        "# than the 100 I originally extracted.\n",
        "# NOTE: This code only needs to run once per dataset, to prevent saving duplicate .txt files by accident.\n",
        "\n",
        "import fitz #for PyMuPDF\n",
        "from tqdm import tqdm #for progress bar\n",
        "\n",
        "# Define a function to extract all text from a PDF using PyMuPDF\n",
        "# Each page's text is joined into a single string.\n",
        "# Returns an empty string and logs an error if the PDF fails to open/read\n",
        "def extract_text_from_pdf(file_path):\n",
        "  try:\n",
        "    doc = fitz.open(file_path)\n",
        "    return \"\\n\".join(page.get_text() for page in doc)\n",
        "  except Exception as e:\n",
        "    logging.error(f\"Error reading {file_path}: {e}\")\n",
        "    return \"\"\n",
        "\n",
        "# Next I process and extract text from a batch of 100 PDFs only\n",
        "# Again, this is to speed up testing as this is only a mini project\n",
        "\n",
        "pdf_files = sorted(list(pdf_input_folder.glob(\"*.pdf\")))[:500]\n",
        "\n",
        "# Save each extracted document as a .txt file in the output folder\n",
        "\n",
        "for pdf_file in tqdm(pdf_files, desc=\"Extracting text\"):\n",
        "  text = extract_text_from_pdf(pdf_file)\n",
        "  output_file = output_folder / f\"{pdf_file.stem}.txt\"\n",
        "  with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "  logging.info(f\"‚úÖ Saved extracted text: {output_file.name}\")"
      ],
      "metadata": {
        "id": "AIOjdWtlDUko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üßπ Download NLTK Resources\n",
        "\n",
        "Download NLTK tokenizers and stopword lists.\n",
        "\n",
        "**Only needs to be done once** per runtime session.\n"
      ],
      "metadata": {
        "id": "qwh58xYTBTqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK resources\n",
        "# - punkt: for sentence and word tokenisation\n",
        "# - stopwords: for filtering out common words in topic modelling\n",
        "\n",
        "# NOTE: Only do this once.\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt_tab\")"
      ],
      "metadata": {
        "id": "JmZ5oytX05ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÇ Load Text Files into Memory\n",
        "\n",
        "Load all extracted `.txt` files into Python memory for preprocessing and topic modelling.\n",
        "\n",
        "Each document becomes a text string ready for cleaning and analysis.\n"
      ],
      "metadata": {
        "id": "OXcOkyv5BVQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read all .txt files into memory for analysis\n",
        "# I am using pre-extracted PDFs from an earlier project\n",
        "\n",
        "documents = []\n",
        "filenames = []\n",
        "\n",
        "for text_file in tqdm(sorted(output_folder.glob(\"*.txt\")), desc=\"Loading text files\"):\n",
        "  with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "    documents.append(text)\n",
        "    filenames.append(text_file.name)\n",
        "\n",
        "logging.info(f\"Loaded {len(documents)} documents for topic modelling.\")"
      ],
      "metadata": {
        "id": "wYK40LUB1JTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üßπ Preprocess Text Data for LDA\n",
        "\n",
        "This section:\n",
        "- Tokenizes each document (splits into individual words)\n",
        "- Removes:\n",
        "  - Stopwords\n",
        "  - Non-alphabetic tokens\n",
        "  - Rare or meaningless words (custom stopword list, very necessary)\n",
        "\n",
        "The result: clean token lists for each document, ready for modelling.\n"
      ],
      "metadata": {
        "id": "YtTcXrTOBbSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the text (basic preprocessing):\n",
        "# - Convert to lowercase\n",
        "# - Tokenise into small KWs\n",
        "# - Remove stopwords and non-alphabetic tokens\n",
        "# - Remove rare occurences etc.\n",
        "\n",
        "# I am using Gensim for training, Gensim expects (/ functions we will call):\n",
        "# - A dict mapping word IDs to words\n",
        "# - A corpus as a list of Bag-of-wWrd vectors\n",
        "\n",
        "import nltk\n",
        "\n",
        "base_stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Custom stopwords to avoid unhelpful topic/KW inclusion\n",
        "custom_stopwords = {\n",
        "    # Structural / procedural terms\n",
        "    \"section\", \"article\", \"clause\", \"paragraph\", \"subsection\", \"chapter\", \"appendix\", \"exhibit\",\n",
        "\n",
        "    # Legalese connectors\n",
        "    \"herein\", \"hereby\", \"hereof\", \"hereinafter\", \"hereunder\", \"herewith\", \"hereto\",\n",
        "    \"thereof\", \"therein\", \"thereby\", \"thereunder\", \"therewith\", \"thereon\", \"thereafter\", \"therefrom\",\n",
        "    \"whereas\", \"whereof\", \"witnesseth\", \"aforementioned\", \"aforesaid\", \"forthwith\",\n",
        "\n",
        "    # Temporal and procedural glue\n",
        "    \"forth\", \"following\", \"preceding\", \"prior\", \"subsequent\", \"pursuant\", \"notwithstanding\",\n",
        "\n",
        "    # Reference/summary phrases\n",
        "    \"included\", \"includes\", \"including\", \"refer\", \"referred\", \"reference\", \"relating\", \"contained\", \"contain\",\n",
        "    \"provides\", \"provided\", \"consist\", \"comprise\", \"accordingly\", \"respectively\", \"additionally\", \"meanwhile\", \"similarly\",\n",
        "\n",
        "    # Auxiliary / modal verbs\n",
        "    \"must\", \"shall\", \"may\", \"might\", \"should\", \"could\", \"can\", \"cannot\", \"will\", \"would\",\n",
        "\n",
        "    # Single-character filler & incomplete (etc) words\n",
        "    \"b\", \"x\", \"e\", \"i\", \"ii\", \"iii\", \"iv\", \"v\", \"de\", \"des\", \"le\", \"les\", \"et\", \"en\", \"fi\", \"d\", \"l\", \"la\", \"d\",\n",
        "\n",
        "    # Misc empty-content terms & pronouns\n",
        "    \"total\", \"date\", \"number\", \"digits\", \"digit\", \"page\", \"note\", \"notes\", \"notation\", \"noted\", \"they\", \"I\", \"we\", \"she\", \"he\"\n",
        "}\n",
        "\n",
        "# Merge both stopwords into master stopword list\n",
        "all_stopwords = base_stopwords.union(custom_stopwords)\n",
        "\n",
        "def preprocess(text):\n",
        "  tokens = word_tokenize(text.lower())\n",
        "  return [word for word in tokens if word.isalpha() and word not in all_stopwords]\n",
        "\n",
        "# Apply preprocssing to all documents\n",
        "tokenized_docs = [preprocess(doc) for doc in tqdm(documents, desc=\"Tokenising documents\")]\n",
        "\n",
        "# Create BoW corpus and dictionary\n",
        "dictionary = corpora.Dictionary(tokenized_docs)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n"
      ],
      "metadata": {
        "id": "z36p0ao838Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Train LDA Topic Model\n",
        "\n",
        "The below sections build and then run a function to train an LDA model to discover hidden topics across the documents.\n",
        "\n",
        "Key parameters you can adjust:\n",
        "- `num_topics`: Number of topics to discover\n",
        "- `passes`: How many times the model scans the corpus\n",
        "- `no_below` and `no_above`: Controls filtering of rare/frequent words\n",
        "\n",
        "Training outputs:\n",
        "- The LDA model\n",
        "- Dictionary of terms\n",
        "- Bag-of-Words corpus\n"
      ],
      "metadata": {
        "id": "4IGnfaI_BlzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build reusable function to train the LDA model\n",
        "\n",
        "def build_lda_model(tokenized_docs, num_topics=5, no_below=5, no_above=0.5, passes=10, random_state=42):\n",
        "  \"\"\"\n",
        "  Builds and returns the LDA model, corpus, and dict from tokenized docs.\n",
        "\n",
        "  Returns:\n",
        "  - lda_model: trained LDA model\n",
        "  - Corpus: BoW format corpus\n",
        "  -dictionary: Gensim dict mapping tokens to IDs\n",
        "  \"\"\"\n",
        "  from gensim import corpora, models\n",
        "\n",
        "  # 1 Create dict from tokens\n",
        "  dictionary = corpora.Dictionary(tokenized_docs)\n",
        "\n",
        "  # 2 Remove tokens that are too rare / too common\n",
        "  dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
        "\n",
        "  # 3 Convert docs to BoW format\n",
        "  corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "  # 4 Train LDA model\n",
        "  lda_model = models.LdaModel(\n",
        "      corpus=corpus,\n",
        "      id2word=dictionary,\n",
        "      num_topics=num_topics,\n",
        "      random_state=random_state,\n",
        "      passes=passes,\n",
        "  )\n",
        "\n",
        "  return lda_model, corpus, dictionary\n"
      ],
      "metadata": {
        "id": "CFr5KFhGm6x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build LDA model using all-singing function\n",
        "lda_model, corpus, dictionary = build_lda_model(tokenized_docs, num_topics=5)\n",
        "\n",
        "logging.info(\"LDA model trained successfully.\")\n",
        "\n",
        "# Print discovered topics\n",
        "print(\" Discovered Topics & Top KWs: \\n\")\n",
        "for i, topic in lda_model.print_topics():\n",
        "  print(f\"-> Topic {i}: {topic}\")"
      ],
      "metadata": {
        "id": "dJnpNpfhoDuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üè∑Ô∏è Assign Topics to Documents\n",
        "\n",
        "Each document is now:\n",
        "- Assigned its most probable topic\n",
        "- Saved in a CSV (`topic_assignments.csv`) for easy use in dashboards later.\n",
        "\n",
        "This is the backbone of the Streamlit dashboard search and filter system.\n",
        "\n",
        "Before running, remember to:\n",
        "\n",
        "- Replace \"/YOUR/GDRIVE/PATH/HERE\" with the path at which you want to save your csv."
      ],
      "metadata": {
        "id": "KrzkUKy9By0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export topic assignments for each document\n",
        "# Assign each doc its most dominant topic to group similar documents together, etc.\n",
        "\n",
        "doc_topics = []\n",
        "for i, bow in enumerate(corpus):\n",
        "  topic_probs = lda_model.get_document_topics(bow)\n",
        "  dominant_topic = sorted(topic_probs, key=lambda x: -x[1])[0][0]\n",
        "  doc_topics.append({\n",
        "      \"filename\": filenames[i],\n",
        "      \"topic\": dominant_topic,\n",
        "  })\n",
        "\n",
        "  import pandas as pd\n",
        "\n",
        "  # Save the topic assignments as a CSV file\n",
        "  # Convert results into DF using pandas\n",
        "\n",
        "  topic_df = pd.DataFrame(doc_topics)\n",
        "  topic_csv_path = \"/YOUR/GDRIVE/PATH/HERE\"\n",
        "  topic_df.to_csv(topic_csv_path, index=False)\n",
        "\n",
        "  logging.info(f\"Document topic assignments saved to {topic_csv_path}\")"
      ],
      "metadata": {
        "id": "SrOr0WJ46H0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üñãÔ∏è Map Topics to Human-Readable Titles\n",
        "\n",
        "Instead of displaying vague topic IDs (Topic 0, Topic 1, etc.),  the modules in this step map each topic to a **meaningful descriptive label** based on its top keywords, before saving them to an updated CSV.\n",
        "\n",
        "Outputs:\n",
        "- `topic_label_map.csv`\n",
        "- Updated `topic_assignments.csv` with human-friendly names\n",
        "\n",
        "Note: You will need to run this twice if you are using your own dataset, so you can fetch the top 5 KWs the first time, and then add human-friendly readable topic titles the second time.\n",
        "\n",
        "Before running, remember to:\n",
        "\n",
        "- Replace \"/YOUR/GDRIVE/PATH/HERE\" with the path at which you want to save your topic label map."
      ],
      "metadata": {
        "id": "CNAJdPVdChz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mapping of the topic ID to display descriptibe KW labels\n",
        "\n",
        "def format_topic_label(topic_id, lda_model, top_n=5):\n",
        "  keywords = [word for word, I in  lda_model.show_topic(topic_id, topn=top_n)]\n",
        "  return f\"Topic {topic_id}: \" + \", \".join(keywords)\n",
        "\n",
        "# Build a label map dict\n",
        "\n",
        "topic_labels = {i: format_topic_label(i, lda_model) for i in range(lda_model.num_topics)}\n",
        "\n",
        "# Replace topic IDs in topic_df\n",
        "topic_df[\"topic_label\"] = topic_df[\"topic\"].map(topic_labels)\n",
        "\n",
        "# Human friendly topic titles for topic labels - Note these human friendly titles might need changing and then the code running again if iteratively testing\n",
        "# and getting different words\n",
        "topic_title_map = {\n",
        "    \"Topic 0: water, system, program, control, systems\": \"üåä Environmental Legislation & Infrastructure\",\n",
        "    \"Topic 1: merchant, data, wholesale, goods, humanitarian\": \"üõí Merchandise Data & Humanitarian Commerce\",\n",
        "    \"Topic 2: school, donn√©es, state, students, district\": \"üè´ School District Analysis & Policy Metrics\", #Note: donn√©es means data in French\n",
        "    \"Topic 3: state, act, transportation, secretary, states\": \"üìú State-Level Transport Policy & Administration\",\n",
        "    \"Topic 4: amendment, bank, student, new, virus\": \"üè¶ Financial Policy, Education, & Pandemic Response\"\n",
        "}\n",
        "\n",
        "# Add new column with better theme names\n",
        "topic_df[\"topic_label\"] = topic_df[\"topic_label\"].map(topic_title_map)\n",
        "\n",
        "# For debugging:\n",
        "\n",
        "print(topic_df.columns.tolist())\n",
        "\n",
        "print(\"Discovered Topics & Top KWs: \\n\")\n",
        "for topic_id, label in topic_labels.items():\n",
        "  print(f\" -> {label}\")\n",
        "\n",
        "topic_df.to_csv(topic_csv_path, index=False)\n",
        "\n",
        "logging.info(f\"New document topic mapping saved to {topic_csv_path}\")"
      ],
      "metadata": {
        "id": "M9g8Iqg8WA6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to export a topic-to-label CSV for more readable data headings in html\n",
        "\n",
        "def export_topic_label_map(topic_labels, topic_title_map, save_path):\n",
        "\n",
        "  \"\"\"\n",
        "  Export a CSV mapping of tpic IDs, KW labels, and human-readable topic titles\n",
        "\n",
        "  Args:\n",
        "  - topic_labels (dict): topic_id -> kw string\n",
        "  - topic_title_map (dict): kw string -> readable title\n",
        "  - save-path (str or Path): path to save the CSV\n",
        "  \"\"\"\n",
        "\n",
        "  import pandas as pd\n",
        "\n",
        "  data = []\n",
        "  for topic_id, keyword_label in topic_labels.items():\n",
        "    readable_title = topic_title_map.get(keyword_label, \"\")\n",
        "    data.append({\n",
        "        \"topic_id\": topic_id,\n",
        "        \"keyword_label\": keyword_label,\n",
        "        \"readable_title\": readable_title\n",
        "    })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(save_path, index=False)\n",
        "    print(f\"Topic label mapping exported to {save_path}\")\n"
      ],
      "metadata": {
        "id": "cQqeXpPS5vWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the topic label map\n",
        "\n",
        "export_topic_label_map(\n",
        "    topic_labels,\n",
        "    topic_title_map,\n",
        "    \"/YOUR/GDRIVE/PATH/HERE\"\n",
        ")"
      ],
      "metadata": {
        "id": "A5BVwDvM7ME1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the Discovered topics\n",
        "# Display text for most relevant KWs in each topic\n",
        "# This is useful for interpreting what each topic represents\n",
        "\n",
        "print(\"Topics and topic titles:\\n\")\n",
        "for i, topic in lda_model.print_topics():\n",
        "    label = topic_labels[i]\n",
        "    readable_title = topic_title_map.get(label, \"Unnamed Topic\")\n",
        "    print(f\"-> {readable_title} ({label})\")"
      ],
      "metadata": {
        "id": "fn_PKrfR5Shv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Save pyLDAvis Interactive Visualization\n",
        "\n",
        "Save a fully interactive pyLDAvis HTML visualization showing:\n",
        "- Each topic's position\n",
        "- Top keywords\n",
        "- Topic relevance and relationships\n",
        "\n",
        "This HTML will later be accessible inside the Streamlit app.\n",
        "\n",
        "Before running, remember to:\n",
        "\n",
        "- Replace \"/YOUR/GDRIVE/PATH/HERE\" with the path at which you want to save your pyLDAvis.html"
      ],
      "metadata": {
        "id": "hVy6CpudDKIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise the topic\n",
        "# Create an interactive visualisation including:\n",
        "# - Topic circles representing clusters\n",
        "# - Right pane showing top words per topic\n",
        "\n",
        "# Prepare the pyLDAvis visualisation data object\n",
        "vis_data = gensimvis.prepare (lda_model, corpus, dictionary)\n",
        "\n",
        "\n",
        "pyLDAvis.save_html(vis_data, \"/YOUR/GDRIVE/PATH/HERE\")\n",
        "logging.info(\"pyLDAvis HTML is saved.\")\n",
        "\n",
        "pyLDAvis.display(vis_data)\n"
      ],
      "metadata": {
        "id": "3fsBV73Z5ki2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üñ•Ô∏è Build the Streamlit Dashboard\n",
        "\n",
        "This final section automatically generates a ready-to-run Streamlit app:\n",
        "- Topic filtering\n",
        "- Full-text keyword search\n",
        "- pyLDAvis visualization\n",
        "- Human-readable topic names\n",
        "\n",
        "‚úÖ You can now deploy your LDA Topic Modelling project as an interactive web dashboard!\n",
        "\n",
        "Before running, remember to:\n",
        "\n",
        "- Replace \"/YOUR/GDRIVE/PATH/HERE\" with the path at which you want to save your streamlit .py file."
      ],
      "metadata": {
        "id": "CoLgAlDfDPRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to create a Streamlit dashboard for topic modelling\n",
        "# First, define Streamlit dashboard layout and logic\n",
        "\n",
        "streamlit_script = r\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import os\n",
        "import re\n",
        "import webbrowser\n",
        "\n",
        "# Set up Streamlit\n",
        "st.set_page_config(page_title=\"Topic Modelling Dashboard\", page_icon=\"üìä\", layout=\"wide\")\n",
        "\n",
        "# Define base path relative to location of the app folder\n",
        "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "csv_path = os.path.join(BASE_DIR, \"topic_assignments.csv\")\n",
        "label_map_path = os.path.join(BASE_DIR, \"topic_label_map.csv\")\n",
        "text_folder = os.path.join(BASE_DIR, \"extracted_texts\")\n",
        "pyldavis_path = os.path.join(BASE_DIR, \"lda_topic_dashboard.html\")\n",
        "\n",
        "# Data load\n",
        "df = pd.read_csv(csv_path, encoding=\"utf-8\")  # Force correct UTF-8 decoding for emoji characters\n",
        "\n",
        "# Load labels\n",
        "if os.path.exists(label_map_path):\n",
        "    label_map = pd.read_csv(label_map_path)\n",
        "else:\n",
        "    label_map = None\n",
        "    st.warning(f\"Topic label map file not found: {label_map_path}\")\n",
        "\n",
        "# UI heading\n",
        "st.title(\"üìä Topic Modelling Dashboard\")\n",
        "st.markdown(\"Explore topics discovered in synthetic documents.\")\n",
        "\n",
        "# Topic selection\n",
        "selected_theme = st.selectbox(\"Select a topic to explore:\", sorted(df['topic_label'].dropna().unique()))\n",
        "\n",
        "# Theme filter\n",
        "filtered_df = df[df['topic_label'] == selected_theme]\n",
        "\n",
        "st.subheader(f\"Topic: {selected_theme}\")\n",
        "\n",
        "if not filtered_df.empty:\n",
        "    st.write(f\"Showing {len(filtered_df)} documents related to this theme.\")\n",
        "    st.dataframe(filtered_df[['filename', 'topic_label']])\n",
        "else:\n",
        "    st.warning(\"üõë No documents found for this theme.\")\n",
        "\n",
        "# KW search (an adapted vers of user Kaushik000raj's search code) ---\n",
        "search_term = st.text_input(\"üîç Search for a keyword or phrase inside all document contents (regex supported):\")\n",
        "\n",
        "if search_term:\n",
        "    st.markdown(f\"Searching for **'{search_term}'**...\")\n",
        "    matches = []\n",
        "\n",
        "    for root, dirs, files in os.walk(text_folder):\n",
        "        for fname in files:\n",
        "            if fname.endswith(\".txt\"):\n",
        "                full_path = os.path.join(root, fname)\n",
        "\n",
        "                try:\n",
        "                    with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                        content = f.read()\n",
        "                        match = re.search(search_term, content, re.IGNORECASE)\n",
        "                        if match:\n",
        "                            snippet_start = max(0, match.start() - 100)\n",
        "                            snippet = content[snippet_start:match.end() + 100]\n",
        "                            highlighted = re.sub(search_term, f\"**{match.group()}**\", snippet, flags=re.IGNORECASE)\n",
        "\n",
        "                            match_row = {\n",
        "                                \"filename\": fname,  # match using .txt filename\n",
        "                                \"snippet\": highlighted\n",
        "                            }\n",
        "\n",
        "                            topic_row = df[df['filename'] == fname]\n",
        "                            if not topic_row.empty:\n",
        "                                match_row[\"topic_label\"] = topic_row.iloc[0]['topic_label']\n",
        "\n",
        "                            matches.append(match_row)\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error reading {fname}: {e}\")\n",
        "\n",
        "    # Search result display\n",
        "    if matches:\n",
        "        st.success(f\"‚úÖ Found {len(matches)} documents containing the term.\")\n",
        "        for row in matches:\n",
        "            st.markdown(f\"üìÑ **{row['filename']}** ‚Äî _{row.get('topic_label', 'No label')}_\")\n",
        "            st.markdown(row[\"snippet\"], unsafe_allow_html=True)\n",
        "    else:\n",
        "        st.warning(\"üõë No matches found.\")\n",
        "\n",
        "# Sidebar ref for themes\n",
        "if label_map is not None:\n",
        "    st.sidebar.title(\"üìò Topic Theme Reference\")\n",
        "    for _, row in label_map.iterrows():\n",
        "        st.sidebar.markdown(f\"**Topic {row['topic_id']}**: {row['readable_title']}\")\n",
        "\n",
        "# Button to open pyLDAvis map\n",
        "if st.button(\"Open pyLDAvis topic map in browser\"):\n",
        "    if os.path.exists(pyldavis_path):\n",
        "        webbrowser.open_new_tab(pyldavis_path)\n",
        "    else:\n",
        "        st.error(\"Topic map file not found.\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Save the Streamlit .py script to GDrive\n",
        "\n",
        "output_path = \"/YOUR/GDRIVE/PATH/HERE\"\n",
        "\n",
        "with open(output_path, \"w\") as f:\n",
        "  f.write(streamlit_script)\n",
        "\n",
        "print(f\"Streamlit app saved to {output_path}\")"
      ],
      "metadata": {
        "id": "5h3gUaVtN7tC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}